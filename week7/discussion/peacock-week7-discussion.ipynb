{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#12.1\r\n",
    "\r\n",
    "import requests\r\n",
    "\r\n",
    "response = requests.get('https://www.python.org')\r\n",
    "\r\n",
    "response.content\r\n",
    "\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "\r\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\r\n",
    "\r\n",
    "text = soup.get_text(strip = True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#12.2\r\n",
    "\r\n",
    "from textblob import TextBlob\r\n",
    "\r\n",
    "blob = TextBlob(text)\r\n",
    "\r\n",
    "sentences = blob.sentences\r\n",
    "words = blob.words\r\n",
    "\r\n",
    "print(blob.noun_phrases)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['welcome', 'python.orgnotice', 'javascript', 'please', 'javascript', 'full experience.skip', 'python networkdonate≡menusearch', 'sitegoaasmallerlargerresetsocializefacebooktwitterchat', 'ircaboutapplicationsquotesgetting startedhelppython brochuredownloadsall', 'releasessource codewindowsmacosother', 'platformslicensealternative implementationsdocumentationdocsaudio/visual talksbeginner', 'guidedeveloper', 'guidefaqnon-english docspep indexpython bookspython essayscommunitydiversitymailing listsircforumspsf annual impact reportpython conferencesspecial interest groupspython logopython wikimerchandisecommunity awardscode', 'conductget involvedshared storiessuccess storiesartsbusinesseducationengineeringgovernmentscientificsoftware developmentnewspython newspsf newslettercommunity newspsf newspycon newseventspython eventsuser', 'eventspython events archiveuser', 'events archivesubmit', 'event', '> _launch', 'interactive shell', 'python', 'fibonacci', 'n > > > def fib', '> > >', 'b =', '> > >', '< n', '> > > print', '> > >', 'b = b', 'a+b > > > print', '> > > fib', 'definedthe', 'python', 'optional arguments', 'keyword arguments', 'arbitrary argument lists.more', 'python', 'python', 'list', \"comprehensions > > > fruits = [ 'banana\", '] > > > loud_fruits = [ fruit.upper', 'fruits ] > > > print', \"[ 'banana\", '] #', 'list', 'enumerate function > > > list', 'compound data typeslists', 'compound data types', 'python', 'lists', 'built-in functions.more', 'python', 'python', 'simple', 'arithmetic > > >', '> > >', '* *', '> > >', 'classic division returns', 'float5.666666666666667 > > >', '# floor division5intuitive', 'interpretationcalculations', 'python', 'expression syntax', '* and/work', 'simple math functions', 'python', '3. #', 'python', 'simple', 'unicode', '> > > print', 'hello', 'python', 'hello', 'python', 'input', 'assignment > > > name = input', '> > > print', '% s', '% name', 'python hi', 'python.quick', 'easy', 'learnexperienced', 'python', 'clean syntax', 'indentation structure', 'python', 'overview. #', 'list > > > numbers = [', '] > > > product =', '> > >', '... product = product * number ... > > > print', \"'the product\", 'flow', '’ d', 'expectpython', 'usual control flow statements', 'own twists', 'course.more control flow tools', 'python', 'moreget startedwhether', 'python.start', 'beginner', '’ s', 'guidedownloadpython', 'source code', 'latest', 'python', 'python', 'standard library', 'python', 'ourrelaunched', 'community-run job boardis', 'newsmore2022-09-12python', 'available2022-09-07python releases 3.10.7', 'available2022-08-08python 3.11.0rc1', 'available2022-08-02python 3.10.6', 'available2022-07-26python 3.11.0b5', 'eventsmore2022-10-05crafting software2022-10-05crafting software series2022-10-08smart iterator challenge', 'mea', 'global devslam', 'za', 'storiesmorepython', 'major aspects', 'abridge', '’ s', 'ml', 'data annotation', 'ml', 'model deployment', 'clinical conversations', 'pythonby nimshi venkat', 'sandeep konamuse python', 'for…moreweb development', 'django', 'pyramid', 'bottle', 'tornado', 'flask', 'web2pygui development', 'pygobject', 'pyqt', 'pyside', 'kivy', 'numeric', 'scipy', 'pandas', 'ipythonsoftware', 'buildbot', 'trac', 'roundupsystem', 'ansible', 'openstack', 'xonsh > > >', 'python enhancement proposals', 'peps', 'pythonis', 'here.rss > > >', 'python software foundationthe', 'python software', 'python', 'international community', 'python', 'programmers.learn morebecome', 'memberdonate', 'psf▲back', 'topaboutapplicationsquotesgetting startedhelppython brochuredownloadsall', 'releasessource codewindowsmacosother', 'platformslicensealternative implementationsdocumentationdocsaudio/visual talksbeginner', 'guidedeveloper', 'guidefaqnon-english docspep indexpython bookspython essayscommunitydiversitymailing listsircforumspsf annual impact reportpython conferencesspecial interest groupspython logopython wikimerchandisecommunity awardscode', 'conductget involvedshared storiessuccess storiesartsbusinesseducationengineeringgovernmentscientificsoftware developmentnewspython newspsf newslettercommunity newspsf newspycon newseventspython eventsuser', 'eventspython events archiveuser', 'events archivesubmit', 'eventcontributingdeveloper', 'guideissue trackerpython-dev', 'mentorshipreport', 'issue▲back', 'tophelp', 'generalcontactdiversityinitiativessubmit website bugstatuscopyright', 'software foundationlegal statementsprivacy policypowered', 'heroku']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#12.3\r\n",
    "response = requests.get('https://towardsdatascience.com/web-scraping-news-articles-in-python-9dd605799558')\r\n",
    "\r\n",
    "response.content\r\n",
    "\r\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\r\n",
    "\r\n",
    "text = soup.get_text(strip = True)\r\n",
    "\r\n",
    "blob = TextBlob(text)\r\n",
    "\r\n",
    "print(blob.sentiment)\r\n",
    "\r\n",
    "for sentence in blob.sentences:\r\n",
    "    print(f\"Sentence: {sentence}\\n\\tSentiment: {sentence.sentiment}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentiment(polarity=0.14783084299213328, subjectivity=0.48358702229669986)\n",
      "Sentence: Web Scraping news articles in Python | by Miguel Fernández Zafra | Towards Data ScienceOpen in appHomeNotificationsListsStoriesWritePublished inTowards Data ScienceMiguel Fernández ZafraFollowJul 9, 2019·8 min read·Member-onlySaveAn end to end Machine Learning projectWeb Scraping news articles in PythonBuilding a web scraping application in Python made simpleSourceThis article is the second of a series in which I will cover thewhole processof developing a machine learning project.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: If you have not read the first one, I strongly encourage you to do ithere.The project involves the creation of areal-time web applicationthat gathers data from several newspapers and shows a summary of the different topics that are being discussed in the news articles.This is achieved with a supervised machine learningclassification modelthat is able to predict the category of a given news article, aweb scraping methodthat gets the latest news from the newspapers, and an interactiveweb applicationthat shows the obtained results to the user.As I explained in thefirstpost of this series, the motivation behind writing these articles is that a lot of the articles or content published on the internet, books or literature regarding data science and machine learning models focus on the modelling part with the training data.\n",
      "\tSentiment: Sentiment(polarity=0.18333333333333332, subjectivity=0.555952380952381)\n",
      "Sentence: However, a machine learning project ismuch morethan that: once you have a trained model, you need to feed new data to it and what is more important, you need to provideuseful insightsto the final user.The whole process is divided in three different posts:Classification model training (link)News articles web scraping (this post)App creation and deployment (link)The github repo can be foundhere.\n",
      "\tSentiment: Sentiment(polarity=0.20606060606060606, subjectivity=0.6590909090909091)\n",
      "Sentence: It includes all the code and a complete report.In thefirstarticle, we developed thetext classification modelin Python, which allowed us to get a certain news article text and predict its category with an overall good accuracy.This post covers the second part: News articles web scraping.\n",
      "\tSentiment: Sentiment(polarity=0.18571428571428572, subjectivity=0.3119047619047619)\n",
      "Sentence: We’ll create a script thatscrapes the latest news articlesfrom different newspapers and stores the text, which will be fed into the model afterwards to get a prediction of its category.\n",
      "\tSentiment: Sentiment(polarity=0.25, subjectivity=0.75)\n",
      "Sentence: We’ll cover it in the following steps:A brief introduction to webpages and HTMLWeb scraping with BeautifulSoup in Python1.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.21666666666666667)\n",
      "Sentence: A brief introduction to webpage design and HTMLIf we want to be able to extract news articles (or, in fact, any other kind of text) from a website, the first step is to know how a websiteworks.\n",
      "\tSentiment: Sentiment(polarity=0.24500000000000002, subjectivity=0.5133333333333334)\n",
      "Sentence: We will follow an example with theTowards Data Sciencewebpage.When we insert anurlinto the web browser (i.e.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: Google Chrome, Firefox, etc…) and access to it, what we see is the combination ofthree technologies:HTML(HyperText Markup Language): it is the standard language for adding content to a website.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: It allows us to insert text, images and other things to our site.\n",
      "\tSentiment: Sentiment(polarity=-0.125, subjectivity=0.375)\n",
      "Sentence: In one word, HTML determines thecontentof a webpage.CSS(Cascading Style Sheets): this language allows us to set the visual design of a website.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: This means, it determines thestyleof a webpage.JavaScript: JavaScript allows us to make the content and the styleinteractive.Note that these three are programming languages.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: They will allow us to create and manipulateevery aspectof the design of a webpage.However, if we want a website to be accessible to every one in a browser, we need to know about additional things: standing up a web server, using a certain domain, etc… But since we are only interested in extracting content from a webpage, this will be enough for today.Let’s illustrate these concepts with an example.\n",
      "\tSentiment: Sentiment(polarity=0.16785714285714287, subjectivity=0.5892857142857142)\n",
      "Sentence: When we visit theTowards Data Sciencehomepage, we see the following:If we deleted theCSScontent from the webpage, we would see something like this:And if we disabledJavaScript, we would not be able to use this pop-up no more:At this point, I’ll ask the following question:“If I want to extract the content of a webpage via web scraping, where do I need to look up?”If your answer was theHTMLcode, then you’re absolutely getting it.\n",
      "\tSentiment: Sentiment(polarity=0.2333333333333333, subjectivity=0.5416666666666666)\n",
      "Sentence: In the above example we can see that after disabling CSS, the content (text, images, etc…) is still there.So, the last step before performing web scraping methods is to understand a bit of theHTML language.HTML is, from a really basic point of view, composed ofelementsthat haveattributes.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.09722222222222222)\n",
      "Sentence: An element could be a paragraph, and an attribute could be that the paragraph is in bold letter.There are a lot of different types of elements, each one with its own attributes.\n",
      "\tSentiment: Sentiment(polarity=0.3111111111111111, subjectivity=0.7555555555555555)\n",
      "Sentence: To identify an element (this means, as an example, to set if some text is a heading or a paragraph) we usetags.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: These tags are represented with the<>symbols (for example, a<p>tag means a certain text is acting as a paragraph).For example, this HTML code below allows us to change the alignment of the paragraphs:Consequently, when we visit a website, we will be able to findthe contentandits propertiesin the HTML code.Once we have presented these concepts, we are ready for some web scraping!2.\n",
      "\tSentiment: Sentiment(polarity=0.2285714285714286, subjectivity=0.42410714285714285)\n",
      "Sentence: Web scraping with BeautifulSoup in PythonThere are several packages in Python that allow us to scrape information from webpages.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: One of the most common ones isBeautifulSoup.\n",
      "\tSentiment: Sentiment(polarity=0.1, subjectivity=0.5)\n",
      "Sentence: The official package information can be foundhere.BeautifulSoup allows us to parse the HTML content of a given URL and access its elements by identifying them with their tags and attributes.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: For this reason, we will use it to extract certain pieces of text from the websites.It is an extremely easy-to-use yet powerful package.\n",
      "\tSentiment: Sentiment(polarity=0.12976190476190474, subjectivity=0.8571428571428571)\n",
      "Sentence: With almost 3–5 lines of code we will be able to extract any text we want from the internet.To install it, please type the following code into your Python distribution:!\n",
      "\tSentiment: Sentiment(polarity=0.25, subjectivity=0.3625)\n",
      "Sentence: pip install beautifulsoup4So as to provide BeautifulSoup with the HTML code of any page, we will also need to import therequestsmodule.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: In order to install it if it's not already included in your python distribution, please type:!\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: pip install requestsWe will use therequestsmodule to get the HTML code from the page and then navigate through it with the BeautifulSoup package.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: We will learn to use two commands that will be enough for our task:find_all(element tag, attribute): it allows us to locate any HTML element from a webpage introducing its tag and attributes.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.5)\n",
      "Sentence: This command will locate all the elements of the same type.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.125)\n",
      "Sentence: In order to get only the first one, we can usefind()instead.get_text(): once we have located a given element, this command will allow us to extract the text inside.So, at this point, what we need to do is tonavigate through the HTML code of our webpage(for example, in Google Chrome we need to enter the webpage, press right click button and go toSee source code) andlocate the elements we want to scrape.\n",
      "\tSentiment: Sentiment(polarity=0.17857142857142858, subjectivity=0.623015873015873)\n",
      "Sentence: We can simply do this searching with Ctrl+F or Cmd+F once we are seeing the source code.Once we have identified the elements of interest, we willget the HTML codewith therequestsmodule andextract those elementswith BeautifulSoup.We will carry out an example with theEl Pais Englishnewspaper.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.35714285714285715)\n",
      "Sentence: We will first try to web scrape the news articles titles from the frontpage and then extract the text out of them.Once we enter the website, we need to inspect the HTML code to locate the news articles.\n",
      "\tSentiment: Sentiment(polarity=0.25, subjectivity=0.3333333333333333)\n",
      "Sentence: After a fast look we can see that each article in the frontpage is an element like this:The title is an<h2>(heading-2) element withitemprop=”headline\"andclass=”articulo-titulo\"atributes.\n",
      "\tSentiment: Sentiment(polarity=0.2, subjectivity=0.6)\n",
      "Sentence: It has an<a>element with anhrefattribute which contains the text.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: So, in order to extract the text, we need to code the following commands:# importing the necessary packagesimport requestsfrom bs4 import BeautifulSoupWith therequestsmodule we can get the HTML content and save into thecoverpagevariable:r1 = requests.get(url)coverpage = r1.contentNext, we need to create asoupin order to allow BeautifulSoup to work:soup1 = BeautifulSoup(coverpage, 'html5lib')And finally, we can locate the elements we are looking for:coverpage_news = soup1.find_all('h2', class_='articulo-titulo')This will return a list in which each element is a news article (because withfind_allwe are getting all ocurrences):If we code the following command, we will be able to extract the text:coverpage_news[4].get_text()If we want to access the value of an attribute (in this case, the link), we can type the following:coverpage_news[4]['href']And we’ll get the link in plain text.If you have understood until this point, you are ready to web scrapeany contentyou want.The next step would be to access each of the news articles content with thehrefattribute, get the source code again and find the paragraphs in the HTML code to finally get them with BeautifulSoup.\n",
      "\tSentiment: Sentiment(polarity=0.05396825396825397, subjectivity=0.5202380952380952)\n",
      "Sentence: It’s the same idea as before, but we need to locate the tags and attributes that identify the news article content.The code of the full process is the following.\n",
      "\tSentiment: Sentiment(polarity=0.11666666666666665, subjectivity=0.25833333333333336)\n",
      "Sentence: I will show the code but won’t enter in the same detail as before since it’s exactly the same idea.# Scraping the first 5 articlesnumber_of_articles = 5# Empty lists for content, links and titlesnews_contents = []list_links = []list_titles = []for n in np.arange(0, number_of_articles):# only news articles (there are also albums and other things)if \"inenglish\" not in coverpage_news[n].find('a')['href']:continue# Getting the link of the articlelink = coverpage_news[n].find('a')['href']list_links.append(link)# Getting the titletitle = coverpage_news[n].find('a').get_text()list_titles.append(title)# Reading the content (it is divided in paragraphs)article = requests.get(link)article_content = article.contentsoup_article = BeautifulSoup(article_content, 'html5lib')body = soup_article.find_all('div', class_='articulo-cuerpo')x = body[0].find_all('p')# Unifying the paragraphslist_paragraphs = []for p in np.arange(0, len(x)):paragraph = x[p].get_text()list_paragraphs.append(paragraph)final_article = \" \".join(list_paragraphs)news_contents.append(final_article)All the details can be found in my githubrepo.It is important to mention that this code is onlyuseful for this webpagein particular.\n",
      "\tSentiment: Sentiment(polarity=0.09351851851851851, subjectivity=0.449074074074074)\n",
      "Sentence: If we want to scrape another one, weshould expectthat elements are identified withdifferent tags and attributes.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: But once we know how to identify them, the process is exactly the same.At this point, we are able to extract the content of different news articles.\n",
      "\tSentiment: Sentiment(polarity=0.25, subjectivity=0.4916666666666667)\n",
      "Sentence: The final step is to apply the machine learning model we trained in thefirst postto predict its categories and show a summary to the user.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=1.0)\n",
      "Sentence: This will be covered in the final post of this series.----11More from Towards Data ScienceFollowYour home for data science.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=1.0)\n",
      "Sentence: A Medium publication sharing concepts, ideas and codes.Read more fromTowards Data ScienceRecommended from MediumShaun NealThe need for energy efficient proof of work algorithmsAmlan BiswasHow to start coding?\n",
      "\tSentiment: Sentiment(polarity=0.5, subjectivity=0.5)\n",
      "Sentence: No.\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: Wait!\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentence: Why start coding?Sandeep KumarWhat is MicroService?BharathirajainAmpersand AcademyGenerate QR Code Using PHPNuno BispoinGeek CultureBuilding a FARM Stack Application — Part 2 — Creating an APIPalySquare 玩贰+“玩贰+” 二次元社交元宇宙Subhajit SahuLinear programmingRakesh MinTowards AWSPart 3 — Interface Endpoints — Testing with S3 and Workspaces as services — Practically…AboutHelpTermsPrivacyGet the Medium appGet startedMiguel Fernández Zafra657 FollowersPassionate about Finance and Data Science, and looking forward to combining these two worlds so as to take advantage of what technology can bring to us.FollowHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable\n",
      "\tSentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#12.4\r\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\r\n",
    "\r\n",
    "blob = TextBlob(text, analyzer=NaiveBayesAnalyzer())\r\n",
    "\r\n",
    "for sentence in blob.sentences:\r\n",
    "    print(f\"Sentence: {sentence}\\n\\tSentiment: {sentence.sentiment}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence: Web Scraping news articles in Python | by Miguel Fernández Zafra | Towards Data ScienceOpen in appHomeNotificationsListsStoriesWritePublished inTowards Data ScienceMiguel Fernández ZafraFollowJul 9, 2019·8 min read·Member-onlySaveAn end to end Machine Learning projectWeb Scraping news articles in PythonBuilding a web scraping application in Python made simpleSourceThis article is the second of a series in which I will cover thewhole processof developing a machine learning project.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.910625990943548, p_neg=0.08937400905645108)\n",
      "Sentence: If you have not read the first one, I strongly encourage you to do ithere.The project involves the creation of areal-time web applicationthat gathers data from several newspapers and shows a summary of the different topics that are being discussed in the news articles.This is achieved with a supervised machine learningclassification modelthat is able to predict the category of a given news article, aweb scraping methodthat gets the latest news from the newspapers, and an interactiveweb applicationthat shows the obtained results to the user.As I explained in thefirstpost of this series, the motivation behind writing these articles is that a lot of the articles or content published on the internet, books or literature regarding data science and machine learning models focus on the modelling part with the training data.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9862757771982703, p_neg=0.013724222801738293)\n",
      "Sentence: However, a machine learning project ismuch morethan that: once you have a trained model, you need to feed new data to it and what is more important, you need to provideuseful insightsto the final user.The whole process is divided in three different posts:Classification model training (link)News articles web scraping (this post)App creation and deployment (link)The github repo can be foundhere.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9992875047083283, p_neg=0.0007124952916683002)\n",
      "Sentence: It includes all the code and a complete report.In thefirstarticle, we developed thetext classification modelin Python, which allowed us to get a certain news article text and predict its category with an overall good accuracy.This post covers the second part: News articles web scraping.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.947473145329985, p_neg=0.05252685467001526)\n",
      "Sentence: We’ll create a script thatscrapes the latest news articlesfrom different newspapers and stores the text, which will be fed into the model afterwards to get a prediction of its category.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.6929789362732045, p_neg=0.3070210637267965)\n",
      "Sentence: We’ll cover it in the following steps:A brief introduction to webpages and HTMLWeb scraping with BeautifulSoup in Python1.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.8969491217978768, p_neg=0.10305087820212303)\n",
      "Sentence: A brief introduction to webpage design and HTMLIf we want to be able to extract news articles (or, in fact, any other kind of text) from a website, the first step is to know how a websiteworks.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.5087656152987517, p_neg=0.4912343847012504)\n",
      "Sentence: We will follow an example with theTowards Data Sciencewebpage.When we insert anurlinto the web browser (i.e.\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.28859340873476536, p_neg=0.711406591265234)\n",
      "Sentence: Google Chrome, Firefox, etc…) and access to it, what we see is the combination ofthree technologies:HTML(HyperText Markup Language): it is the standard language for adding content to a website.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.934100890889485, p_neg=0.06589910911051564)\n",
      "Sentence: It allows us to insert text, images and other things to our site.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.8634892613109523, p_neg=0.13651073868904917)\n",
      "Sentence: In one word, HTML determines thecontentof a webpage.CSS(Cascading Style Sheets): this language allows us to set the visual design of a website.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9221359967554446, p_neg=0.07786400324455413)\n",
      "Sentence: This means, it determines thestyleof a webpage.JavaScript: JavaScript allows us to make the content and the styleinteractive.Note that these three are programming languages.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9454349520671351, p_neg=0.054565047932862334)\n",
      "Sentence: They will allow us to create and manipulateevery aspectof the design of a webpage.However, if we want a website to be accessible to every one in a browser, we need to know about additional things: standing up a web server, using a certain domain, etc… But since we are only interested in extracting content from a webpage, this will be enough for today.Let’s illustrate these concepts with an example.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.965344070205334, p_neg=0.03465592979466713)\n",
      "Sentence: When we visit theTowards Data Sciencehomepage, we see the following:If we deleted theCSScontent from the webpage, we would see something like this:And if we disabledJavaScript, we would not be able to use this pop-up no more:At this point, I’ll ask the following question:“If I want to extract the content of a webpage via web scraping, where do I need to look up?”If your answer was theHTMLcode, then you’re absolutely getting it.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.5494127598890857, p_neg=0.4505872401109131)\n",
      "Sentence: In the above example we can see that after disabling CSS, the content (text, images, etc…) is still there.So, the last step before performing web scraping methods is to understand a bit of theHTML language.HTML is, from a really basic point of view, composed ofelementsthat haveattributes.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9779362922911445, p_neg=0.022063707708859612)\n",
      "Sentence: An element could be a paragraph, and an attribute could be that the paragraph is in bold letter.There are a lot of different types of elements, each one with its own attributes.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.6813887442742685, p_neg=0.31861125572572996)\n",
      "Sentence: To identify an element (this means, as an example, to set if some text is a heading or a paragraph) we usetags.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9037110326353255, p_neg=0.09628896736467266)\n",
      "Sentence: These tags are represented with the<>symbols (for example, a<p>tag means a certain text is acting as a paragraph).For example, this HTML code below allows us to change the alignment of the paragraphs:Consequently, when we visit a website, we will be able to findthe contentandits propertiesin the HTML code.Once we have presented these concepts, we are ready for some web scraping!2.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9959855663030049, p_neg=0.00401443369699776)\n",
      "Sentence: Web scraping with BeautifulSoup in PythonThere are several packages in Python that allow us to scrape information from webpages.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.7892203999858511, p_neg=0.2107796000141515)\n",
      "Sentence: One of the most common ones isBeautifulSoup.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.7183259796163747, p_neg=0.28167402038362543)\n",
      "Sentence: The official package information can be foundhere.BeautifulSoup allows us to parse the HTML content of a given URL and access its elements by identifying them with their tags and attributes.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.8228081718509286, p_neg=0.17719182814907108)\n",
      "Sentence: For this reason, we will use it to extract certain pieces of text from the websites.It is an extremely easy-to-use yet powerful package.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.7509767400720407, p_neg=0.2490232599279606)\n",
      "Sentence: With almost 3–5 lines of code we will be able to extract any text we want from the internet.To install it, please type the following code into your Python distribution:!\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.42237405706501835, p_neg=0.577625942934977)\n",
      "Sentence: pip install beautifulsoup4So as to provide BeautifulSoup with the HTML code of any page, we will also need to import therequestsmodule.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.666331479993451, p_neg=0.3336685200065484)\n",
      "Sentence: In order to install it if it's not already included in your python distribution, please type:!\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.6680882566691695, p_neg=0.33191174333083023)\n",
      "Sentence: pip install requestsWe will use therequestsmodule to get the HTML code from the page and then navigate through it with the BeautifulSoup package.\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.4709261118889735, p_neg=0.5290738881110268)\n",
      "Sentence: We will learn to use two commands that will be enough for our task:find_all(element tag, attribute): it allows us to locate any HTML element from a webpage introducing its tag and attributes.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.7563659251523269, p_neg=0.24363407484766927)\n",
      "Sentence: This command will locate all the elements of the same type.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.7685176312321956, p_neg=0.2314823687678037)\n",
      "Sentence: In order to get only the first one, we can usefind()instead.get_text(): once we have located a given element, this command will allow us to extract the text inside.So, at this point, what we need to do is tonavigate through the HTML code of our webpage(for example, in Google Chrome we need to enter the webpage, press right click button and go toSee source code) andlocate the elements we want to scrape.\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.46516758078392834, p_neg=0.5348324192160718)\n",
      "Sentence: We can simply do this searching with Ctrl+F or Cmd+F once we are seeing the source code.Once we have identified the elements of interest, we willget the HTML codewith therequestsmodule andextract those elementswith BeautifulSoup.We will carry out an example with theEl Pais Englishnewspaper.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.8528300262277866, p_neg=0.14716997377221316)\n",
      "Sentence: We will first try to web scrape the news articles titles from the frontpage and then extract the text out of them.Once we enter the website, we need to inspect the HTML code to locate the news articles.\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.01880952580431125, p_neg=0.9811904741956871)\n",
      "Sentence: After a fast look we can see that each article in the frontpage is an element like this:The title is an<h2>(heading-2) element withitemprop=”headline\"andclass=”articulo-titulo\"atributes.\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.3586029483777541, p_neg=0.641397051622246)\n",
      "Sentence: It has an<a>element with anhrefattribute which contains the text.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.6551233675849583, p_neg=0.3448766324150412)\n",
      "Sentence: So, in order to extract the text, we need to code the following commands:# importing the necessary packagesimport requestsfrom bs4 import BeautifulSoupWith therequestsmodule we can get the HTML content and save into thecoverpagevariable:r1 = requests.get(url)coverpage = r1.contentNext, we need to create asoupin order to allow BeautifulSoup to work:soup1 = BeautifulSoup(coverpage, 'html5lib')And finally, we can locate the elements we are looking for:coverpage_news = soup1.find_all('h2', class_='articulo-titulo')This will return a list in which each element is a news article (because withfind_allwe are getting all ocurrences):If we code the following command, we will be able to extract the text:coverpage_news[4].get_text()If we want to access the value of an attribute (in this case, the link), we can type the following:coverpage_news[4]['href']And we’ll get the link in plain text.If you have understood until this point, you are ready to web scrapeany contentyou want.The next step would be to access each of the news articles content with thehrefattribute, get the source code again and find the paragraphs in the HTML code to finally get them with BeautifulSoup.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9981309616894136, p_neg=0.0018690383105823868)\n",
      "Sentence: It’s the same idea as before, but we need to locate the tags and attributes that identify the news article content.The code of the full process is the following.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.7061877255463428, p_neg=0.293812274453655)\n",
      "Sentence: I will show the code but won’t enter in the same detail as before since it’s exactly the same idea.# Scraping the first 5 articlesnumber_of_articles = 5# Empty lists for content, links and titlesnews_contents = []list_links = []list_titles = []for n in np.arange(0, number_of_articles):# only news articles (there are also albums and other things)if \"inenglish\" not in coverpage_news[n].find('a')['href']:continue# Getting the link of the articlelink = coverpage_news[n].find('a')['href']list_links.append(link)# Getting the titletitle = coverpage_news[n].find('a').get_text()list_titles.append(title)# Reading the content (it is divided in paragraphs)article = requests.get(link)article_content = article.contentsoup_article = BeautifulSoup(article_content, 'html5lib')body = soup_article.find_all('div', class_='articulo-cuerpo')x = body[0].find_all('p')# Unifying the paragraphslist_paragraphs = []for p in np.arange(0, len(x)):paragraph = x[p].get_text()list_paragraphs.append(paragraph)final_article = \" \".join(list_paragraphs)news_contents.append(final_article)All the details can be found in my githubrepo.It is important to mention that this code is onlyuseful for this webpagein particular.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.999957858193073, p_neg=4.2141806927571645e-05)\n",
      "Sentence: If we want to scrape another one, weshould expectthat elements are identified withdifferent tags and attributes.\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.12732193312789752, p_neg=0.8726780668721039)\n",
      "Sentence: But once we know how to identify them, the process is exactly the same.At this point, we are able to extract the content of different news articles.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.8005008200581187, p_neg=0.199499179941879)\n",
      "Sentence: The final step is to apply the machine learning model we trained in thefirst postto predict its categories and show a summary to the user.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9528367662424466, p_neg=0.04716323375755304)\n",
      "Sentence: This will be covered in the final post of this series.----11More from Towards Data ScienceFollowYour home for data science.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.5687707909324469, p_neg=0.43122920906755163)\n",
      "Sentence: A Medium publication sharing concepts, ideas and codes.Read more fromTowards Data ScienceRecommended from MediumShaun NealThe need for energy efficient proof of work algorithmsAmlan BiswasHow to start coding?\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9492939795549409, p_neg=0.05070602044506333)\n",
      "Sentence: No.\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentence: Wait!\n",
      "\tSentiment: Sentiment(classification='neg', p_pos=0.38, p_neg=0.62)\n",
      "Sentence: Why start coding?Sandeep KumarWhat is MicroService?BharathirajainAmpersand AcademyGenerate QR Code Using PHPNuno BispoinGeek CultureBuilding a FARM Stack Application — Part 2 — Creating an APIPalySquare 玩贰+“玩贰+” 二次元社交元宇宙Subhajit SahuLinear programmingRakesh MinTowards AWSPart 3 — Interface Endpoints — Testing with S3 and Workspaces as services — Practically…AboutHelpTermsPrivacyGet the Medium appGet startedMiguel Fernández Zafra657 FollowersPassionate about Finance and Data Science, and looking forward to combining these two worlds so as to take advantage of what technology can bring to us.FollowHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable\n",
      "\tSentiment: Sentiment(classification='pos', p_pos=0.9948898238435083, p_neg=0.005110176156486837)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#12.5\r\n",
    "\r\n",
    "response = requests.get('https://www.gutenberg.org/cache/epub/10947/pg10947.txt')\r\n",
    "\r\n",
    "data = response.text\r\n",
    "\r\n",
    "blob = TextBlob(data)\r\n",
    "\r\n",
    "words = blob.words\r\n",
    "\r\n",
    "#only did the first 1000 words to keep the execution time down\r\n",
    "for word in words[0:1000]:\r\n",
    "    if(word != word.correct()):\r\n",
    "        print(f\"{word}:\\t{word.spellcheck()}\")\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eBook:\t[('ebook', 1.0)]\n",
      "This:\t[('His', 0.7117826487905228), ('This', 0.28821735120947717)]\n",
      "eBook:\t[('ebook', 1.0)]\n",
      "re-use:\t[('refuse', 1.0)]\n",
      "eBook:\t[('ebook', 1.0)]\n",
      "If:\t[('Of', 0.9362354096980188), ('If', 0.05550747350939159), ('Ff', 0.00444434048326355), ('F', 0.0035554723866108393), ('Cf', 0.0002573039227152581)]\n",
      "eBook:\t[('ebook', 1.0)]\n",
      "Date:\t[('Late', 0.3319919517102616), ('Fate', 0.19718309859154928), ('Rate', 0.13480885311871227), ('Gate', 0.13078470824949698), ('Date', 0.096579476861167), ('Ate', 0.04225352112676056), ('Hate', 0.04024144869215292), ('Mate', 0.01006036217303823), ('Kate', 0.01006036217303823), ('Tate', 0.002012072434607646), ('Sate', 0.002012072434607646), ('Pate', 0.002012072434607646)]\n",
      "eBook:\t[('ebook', 1.0)]\n",
      "Etext:\t[('Text', 0.8125), ('Etext', 0.1875)]\n",
      "PG:\t[('of', 0.17512732555086896), ('to', 0.12586415107548524), ('in', 0.0964786390605037), ('a', 0.09256261266779844), ('he', 0.05425993664350596), ('it', 0.046734165251938325), ('is', 0.04276563347743144), ('as', 0.0352836165706985), ('i', 0.03363407249243047), ('at', 0.029713670651241753), ('by', 0.029481771881617866), ('on', 0.02906610427568826), ('be', 0.026930885415755116), ('s', 0.024660027652834416), ('or', 0.023417400283529063), ('an', 0.01497716015891629), ('so', 0.01320072807462765), ('if', 0.010382939251273255), ('no', 0.010273553039186516), ('up', 0.009993524336244465), ('my', 0.009840383639323031), ('me', 0.008400861088261546), ('we', 0.008339604809492973), ('do', 0.006576299070654742), ('t', 0.005766841101212874), ('go', 0.003959780877539948), ('am', 0.0032640845686682887), ('us', 0.002992806762693176), ('ll', 0.0018683165024415002), ('oh', 0.0017939338782225179), ('mr', 0.00157516145404904), ('pp', 0.001347638132908623), ('o', 0.0011244902602516758), ('de', 0.0010369812905822848), ('ah', 0.0009713495633302413), ('ff', 0.0008313352118592156), ('re', 0.000826959763375746), ('d', 0.00078758072702452), ('m', 0.0007438262421898245), ('ve', 0.000669443617970842), ('f', 0.0006650681694873725), ('st', 0.0006431909270700247), ('c', 0.0006256891331361464), ('x', 0.0005950609937518596), ('e', 0.0005950609937518596), ('tm', 0.0005600574058841031), ('j', 0.0004025412604791991), ('eh', 0.00038941491502879046), ('b', 0.00038066401806185134), ('h', 0.0003456604301940949), ('ii', 0.00033690953322715577), ('ha', 0.00032815863626021666), ('w', 0.0003062813938428689), ('l', 0.0002975304968759298), ('p', 0.00028877959990899067), ('n', 0.00027127780597511244), ('g', 0.0002450251150742951), ('le', 0.00024064966659082557), ('la', 0.00024064966659082557), ('iv', 0.00024064966659082557), ('r', 0.00023189876962388645), ('v', 0.00022314787265694734), ('th', 0.00022314787265694734), ('dr', 0.00021002152720653867), ('y', 0.00017064249085531268), ('vi', 0.00016189159388837356), ('k', 0.00014438979995449533), ('ma', 0.00013563890298755622), ('co', 0.00013126345450408666), ('xi', 0.00012251255753714755), ('ix', 0.00012251255753714755), ('u', 0.00010938621208673889), ('en', 0.00010938621208673889), ('xv', 0.00010501076360326933), ('un', 8.313352118592156e-05), ('et', 8.313352118592156e-05), ('il', 7.8758072702452e-05), ('ex', 7.8758072702452e-05), ('au', 7.8758072702452e-05), ('je', 7.438262421898245e-05), ('pa', 6.563172725204333e-05), ('xx', 6.125627876857377e-05), ('ed', 6.125627876857377e-05), ('mb', 5.6880830285104224e-05), ('ne', 5.2505381801634667e-05), ('du', 5.2505381801634667e-05), ('cf', 4.812993331816511e-05), ('ce', 4.812993331816511e-05), ('ax', 4.812993331816511e-05), ('rd', 4.375448483469556e-05), ('hm', 4.375448483469556e-05), ('ze', 3.9379036351226e-05), ('vs', 3.9379036351226e-05), ('oo', 3.9379036351226e-05), ('ho', 3.9379036351226e-05), ('z', 3.5003587867756444e-05), ('ye', 3.5003587867756444e-05), ('qu', 3.5003587867756444e-05), ('op', 3.5003587867756444e-05), ('fe', 3.5003587867756444e-05), ('va', 3.062813938428689e-05), ('ti', 3.062813938428689e-05), ('ku', 3.062813938428689e-05), ('ta', 2.6252690900817333e-05), ('pg', 2.6252690900817333e-05), ('nd', 2.6252690900817333e-05), ('si', 2.187724241734778e-05), ('q', 2.187724241734778e-05), ('fr', 2.187724241734778e-05), ('er', 2.187724241734778e-05), ('em', 2.187724241734778e-05), ('ut', 1.7501793933878222e-05), ('tu', 1.7501793933878222e-05), ('sa', 1.7501793933878222e-05), ('gr', 1.7501793933878222e-05), ('fo', 1.7501793933878222e-05), ('da', 1.7501793933878222e-05), ('se', 1.3126345450408667e-05), ('po', 1.3126345450408667e-05), ('ou', 1.3126345450408667e-05), ('os', 1.3126345450408667e-05), ('na', 1.3126345450408667e-05), ('mi', 1.3126345450408667e-05), ('es', 1.3126345450408667e-05), ('ch', 1.3126345450408667e-05), ('ay', 1.3126345450408667e-05), ('ak', 1.3126345450408667e-05), ('ai', 1.3126345450408667e-05), ('zu', 8.750896966939111e-06), ('wm', 8.750896966939111e-06), ('pe', 8.750896966939111e-06), ('ox', 8.750896966939111e-06), ('oe', 8.750896966939111e-06), ('ni', 8.750896966939111e-06), ('ms', 8.750896966939111e-06), ('ke', 8.750896966939111e-06), ('jr', 8.750896966939111e-06), ('hi', 8.750896966939111e-06), ('ft', 8.750896966939111e-06), ('di', 8.750896966939111e-06), ('cm', 8.750896966939111e-06), ('ba', 8.750896966939111e-06), ('ab', 8.750896966939111e-06), ('wo', 4.3754484834695555e-06), ('wh', 4.3754484834695555e-06), ('wa', 4.3754484834695555e-06), ('vy', 4.3754484834695555e-06), ('vo', 4.3754484834695555e-06), ('tz', 4.3754484834695555e-06), ('ty', 4.3754484834695555e-06), ('tt', 4.3754484834695555e-06), ('tr', 4.3754484834695555e-06), ('te', 4.3754484834695555e-06), ('sq', 4.3754484834695555e-06), ('sn', 4.3754484834695555e-06), ('sg', 4.3754484834695555e-06), ('sd', 4.3754484834695555e-06), ('ri', 4.3754484834695555e-06), ('ra', 4.3754484834695555e-06), ('pm', 4.3754484834695555e-06), ('oz', 4.3754484834695555e-06), ('om', 4.3754484834695555e-06), ('ok', 4.3754484834695555e-06), ('ny', 4.3754484834695555e-06), ('nm', 4.3754484834695555e-06), ('nl', 4.3754484834695555e-06), ('mt', 4.3754484834695555e-06), ('mo', 4.3754484834695555e-06), ('mm', 4.3754484834695555e-06), ('md', 4.3754484834695555e-06), ('ly', 4.3754484834695555e-06), ('lo', 4.3754484834695555e-06), ('li', 4.3754484834695555e-06), ('lb', 4.3754484834695555e-06), ('km', 4.3754484834695555e-06), ('ka', 4.3754484834695555e-06), ('ja', 4.3754484834695555e-06), ('id', 4.3754484834695555e-06), ('ga', 4.3754484834695555e-06), ('fn', 4.3754484834695555e-06), ('ev', 4.3754484834695555e-06), ('el', 4.3754484834695555e-06), ('eg', 4.3754484834695555e-06), ('dy', 4.3754484834695555e-06), ('cy', 4.3754484834695555e-06), ('ca', 4.3754484834695555e-06), ('bu', 4.3754484834695555e-06), ('br', 4.3754484834695555e-06), ('bl', 4.3754484834695555e-06), ('bc', 4.3754484834695555e-06), ('ar', 4.3754484834695555e-06), ('al', 4.3754484834695555e-06), ('ad', 4.3754484834695555e-06), ('ac', 4.3754484834695555e-06)]\n",
      "Widger:\t[('Finger', 0.6192893401015228), ('Wider', 0.08629441624365482), ('Rider', 0.08121827411167512), ('Bigger', 0.03553299492385787), ('Singer', 0.030456852791878174), ('Ledger', 0.030456852791878174), ('Ridge', 0.025380710659898477), ('Tiger', 0.015228426395939087), ('Linger', 0.015228426395939087), ('Ridges', 0.01015228426395939), ('Lodger', 0.01015228426395939), ('Jigger', 0.01015228426395939), ('Nigger', 0.005076142131979695), ('Midges', 0.005076142131979695), ('Idler', 0.005076142131979695), ('Ginger', 0.005076142131979695), ('Cider', 0.005076142131979695), ('Bidder', 0.005076142131979695)]\n",
      "OF:\t[('of', 0.17512732555086896), ('to', 0.12586415107548524), ('in', 0.0964786390605037), ('a', 0.09256261266779844), ('he', 0.05425993664350596), ('it', 0.046734165251938325), ('is', 0.04276563347743144), ('as', 0.0352836165706985), ('i', 0.03363407249243047), ('at', 0.029713670651241753), ('by', 0.029481771881617866), ('on', 0.02906610427568826), ('be', 0.026930885415755116), ('s', 0.024660027652834416), ('or', 0.023417400283529063), ('an', 0.01497716015891629), ('so', 0.01320072807462765), ('if', 0.010382939251273255), ('no', 0.010273553039186516), ('up', 0.009993524336244465), ('my', 0.009840383639323031), ('me', 0.008400861088261546), ('we', 0.008339604809492973), ('do', 0.006576299070654742), ('t', 0.005766841101212874), ('go', 0.003959780877539948), ('am', 0.0032640845686682887), ('us', 0.002992806762693176), ('ll', 0.0018683165024415002), ('oh', 0.0017939338782225179), ('mr', 0.00157516145404904), ('pp', 0.001347638132908623), ('o', 0.0011244902602516758), ('de', 0.0010369812905822848), ('ah', 0.0009713495633302413), ('ff', 0.0008313352118592156), ('re', 0.000826959763375746), ('d', 0.00078758072702452), ('m', 0.0007438262421898245), ('ve', 0.000669443617970842), ('f', 0.0006650681694873725), ('st', 0.0006431909270700247), ('c', 0.0006256891331361464), ('x', 0.0005950609937518596), ('e', 0.0005950609937518596), ('tm', 0.0005600574058841031), ('j', 0.0004025412604791991), ('eh', 0.00038941491502879046), ('b', 0.00038066401806185134), ('h', 0.0003456604301940949), ('ii', 0.00033690953322715577), ('ha', 0.00032815863626021666), ('w', 0.0003062813938428689), ('l', 0.0002975304968759298), ('p', 0.00028877959990899067), ('n', 0.00027127780597511244), ('g', 0.0002450251150742951), ('le', 0.00024064966659082557), ('la', 0.00024064966659082557), ('iv', 0.00024064966659082557), ('r', 0.00023189876962388645), ('v', 0.00022314787265694734), ('th', 0.00022314787265694734), ('dr', 0.00021002152720653867), ('y', 0.00017064249085531268), ('vi', 0.00016189159388837356), ('k', 0.00014438979995449533), ('ma', 0.00013563890298755622), ('co', 0.00013126345450408666), ('xi', 0.00012251255753714755), ('ix', 0.00012251255753714755), ('u', 0.00010938621208673889), ('en', 0.00010938621208673889), ('xv', 0.00010501076360326933), ('un', 8.313352118592156e-05), ('et', 8.313352118592156e-05), ('il', 7.8758072702452e-05), ('ex', 7.8758072702452e-05), ('au', 7.8758072702452e-05), ('je', 7.438262421898245e-05), ('pa', 6.563172725204333e-05), ('xx', 6.125627876857377e-05), ('ed', 6.125627876857377e-05), ('mb', 5.6880830285104224e-05), ('ne', 5.2505381801634667e-05), ('du', 5.2505381801634667e-05), ('cf', 4.812993331816511e-05), ('ce', 4.812993331816511e-05), ('ax', 4.812993331816511e-05), ('rd', 4.375448483469556e-05), ('hm', 4.375448483469556e-05), ('ze', 3.9379036351226e-05), ('vs', 3.9379036351226e-05), ('oo', 3.9379036351226e-05), ('ho', 3.9379036351226e-05), ('z', 3.5003587867756444e-05), ('ye', 3.5003587867756444e-05), ('qu', 3.5003587867756444e-05), ('op', 3.5003587867756444e-05), ('fe', 3.5003587867756444e-05), ('va', 3.062813938428689e-05), ('ti', 3.062813938428689e-05), ('ku', 3.062813938428689e-05), ('ta', 2.6252690900817333e-05), ('pg', 2.6252690900817333e-05), ('nd', 2.6252690900817333e-05), ('si', 2.187724241734778e-05), ('q', 2.187724241734778e-05), ('fr', 2.187724241734778e-05), ('er', 2.187724241734778e-05), ('em', 2.187724241734778e-05), ('ut', 1.7501793933878222e-05), ('tu', 1.7501793933878222e-05), ('sa', 1.7501793933878222e-05), ('gr', 1.7501793933878222e-05), ('fo', 1.7501793933878222e-05), ('da', 1.7501793933878222e-05), ('se', 1.3126345450408667e-05), ('po', 1.3126345450408667e-05), ('ou', 1.3126345450408667e-05), ('os', 1.3126345450408667e-05), ('na', 1.3126345450408667e-05), ('mi', 1.3126345450408667e-05), ('es', 1.3126345450408667e-05), ('ch', 1.3126345450408667e-05), ('ay', 1.3126345450408667e-05), ('ak', 1.3126345450408667e-05), ('ai', 1.3126345450408667e-05), ('zu', 8.750896966939111e-06), ('wm', 8.750896966939111e-06), ('pe', 8.750896966939111e-06), ('ox', 8.750896966939111e-06), ('oe', 8.750896966939111e-06), ('ni', 8.750896966939111e-06), ('ms', 8.750896966939111e-06), ('ke', 8.750896966939111e-06), ('jr', 8.750896966939111e-06), ('hi', 8.750896966939111e-06), ('ft', 8.750896966939111e-06), ('di', 8.750896966939111e-06), ('cm', 8.750896966939111e-06), ('ba', 8.750896966939111e-06), ('ab', 8.750896966939111e-06), ('wo', 4.3754484834695555e-06), ('wh', 4.3754484834695555e-06), ('wa', 4.3754484834695555e-06), ('vy', 4.3754484834695555e-06), ('vo', 4.3754484834695555e-06), ('tz', 4.3754484834695555e-06), ('ty', 4.3754484834695555e-06), ('tt', 4.3754484834695555e-06), ('tr', 4.3754484834695555e-06), ('te', 4.3754484834695555e-06), ('sq', 4.3754484834695555e-06), ('sn', 4.3754484834695555e-06), ('sg', 4.3754484834695555e-06), ('sd', 4.3754484834695555e-06), ('ri', 4.3754484834695555e-06), ('ra', 4.3754484834695555e-06), ('pm', 4.3754484834695555e-06), ('oz', 4.3754484834695555e-06), ('om', 4.3754484834695555e-06), ('ok', 4.3754484834695555e-06), ('ny', 4.3754484834695555e-06), ('nm', 4.3754484834695555e-06), ('nl', 4.3754484834695555e-06), ('mt', 4.3754484834695555e-06), ('mo', 4.3754484834695555e-06), ('mm', 4.3754484834695555e-06), ('md', 4.3754484834695555e-06), ('ly', 4.3754484834695555e-06), ('lo', 4.3754484834695555e-06), ('li', 4.3754484834695555e-06), ('lb', 4.3754484834695555e-06), ('km', 4.3754484834695555e-06), ('ka', 4.3754484834695555e-06), ('ja', 4.3754484834695555e-06), ('id', 4.3754484834695555e-06), ('ga', 4.3754484834695555e-06), ('fn', 4.3754484834695555e-06), ('ev', 4.3754484834695555e-06), ('el', 4.3754484834695555e-06), ('eg', 4.3754484834695555e-06), ('dy', 4.3754484834695555e-06), ('cy', 4.3754484834695555e-06), ('ca', 4.3754484834695555e-06), ('bu', 4.3754484834695555e-06), ('br', 4.3754484834695555e-06), ('bl', 4.3754484834695555e-06), ('bc', 4.3754484834695555e-06), ('ar', 4.3754484834695555e-06), ('al', 4.3754484834695555e-06), ('ad', 4.3754484834695555e-06), ('ac', 4.3754484834695555e-06)]\n",
      "Book:\t[('Took', 0.4377387318563789), ('Look', 0.43315508021390375), ('Book', 0.09549274255156608), ('Cook', 0.020626432391138275), ('Rook', 0.006111535523300229), ('Hook', 0.0053475935828877), ('Nook', 0.0015278838808250573)]\n",
      "Masterpieces:\t[('Masterpiece', 1.0)]\n",
      "This:\t[('His', 0.7117826487905228), ('This', 0.28821735120947717)]\n",
      "Joel:\t[('Noel', 0.5), ('Joel', 0.5)]\n",
      "Chandler:\t[('Handle', 0.5526315789473685), ('Handled', 0.2894736842105263), ('Handley', 0.15789473684210525)]\n",
      "work—a:\t[('work', 0.7074074074074074), ('works', 0.18333333333333332), ('worked', 0.07407407407407407), ('workman', 0.014814814814814815), ('worker', 0.014814814814814815), ('workbag', 0.005555555555555556)]\n",
      "As:\t[('Is', 0.4043019648397104), ('As', 0.33356773526370215), ('S', 0.23313340227507756), ('Us', 0.028293691830403308), ('Vs', 0.00037228541882109616), ('Os', 0.00012409513960703205), ('Es', 0.00012409513960703205), ('Ms', 8.27300930713547e-05)]\n",
      "funniness:\t[('funniest', 1.0)]\n",
      "Cervantes:\t[('Servants', 1.0)]\n",
      "Mark:\t[('Dark', 0.7327935222672065), ('Mark', 0.15384615384615385), ('Park', 0.06072874493927125), ('Bark', 0.03643724696356275), ('Hark', 0.008097165991902834), ('Lark', 0.004048582995951417), ('Ark', 0.004048582995951417)]\n",
      "Twain:\t[('Swain', 0.6666666666666666), ('Twain', 0.3333333333333333)]\n",
      "No:\t[('To', 0.7805182471849138), ('So', 0.0818613485280152), ('No', 0.06370913037579705), ('Do', 0.04078144078144078), ('Go', 0.02455569122235789), ('O', 0.006973273639940307), ('Co', 0.000814000814000814), ('Oo', 0.0002442002442002442), ('Ho', 0.0002442002442002442), ('Fo', 0.0001085334418667752), ('Po', 8.14000814000814e-05), ('Wo', 2.71333604666938e-05), ('Vo', 2.71333604666938e-05), ('Mo', 2.71333604666938e-05), ('Lo', 2.71333604666938e-05)]\n",
      "duller:\t[('fuller', 0.8571428571428571), ('dulled', 0.14285714285714285)]\n",
      "nauseating:\t[('caseating', 1.0)]\n",
      "perused:\t[('pressed', 0.3495702005730659), ('paused', 0.22636103151862463), ('refused', 0.20630372492836677), ('pursued', 0.09455587392550144), ('perished', 0.06303724928366762), ('phrased', 0.014326647564469915), ('perfumed', 0.014326647564469915), ('perched', 0.014326647564469915), ('versed', 0.0057306590257879654), ('wefused', 0.0028653295128939827), ('perusal', 0.0028653295128939827), ('percussed', 0.0028653295128939827), ('erased', 0.0028653295128939827)]\n",
      "assignments:\t[('assignment', 1.0)]\n",
      "ingredients:\t[('ingredient', 1.0)]\n",
      "humorists:\t[('humorist', 1.0)]\n",
      "Mark:\t[('Dark', 0.7327935222672065), ('Mark', 0.15384615384615385), ('Park', 0.06072874493927125), ('Bark', 0.03643724696356275), ('Hark', 0.008097165991902834), ('Lark', 0.004048582995951417), ('Ark', 0.004048582995951417)]\n",
      "Twain:\t[('Swain', 0.6666666666666666), ('Twain', 0.3333333333333333)]\n",
      "Josh:\t[('Most', 0.26871855578573545), ('Both', 0.15655519384433264), ('Oh', 0.12133767386800828), ('Wish', 0.07191476768274638), ('Rose', 0.07191476768274638), ('Lost', 0.06629180230837525), ('Loss', 0.04232021308079313), ('Post', 0.03462562888428529), ('Nose', 0.030482391240011838), ('Cost', 0.019236460491269605), ('Lose', 0.015093222846996152), ('Rush', 0.010358094110683634), ('Rosy', 0.008878366380585973), ('Dose', 0.00739863865048831), ('Host', 0.006806747558449245), ('Pose', 0.006214856466410181), ('Fish', 0.006214856466410181), ('Wash', 0.005918910920390648), ('Push', 0.0056229653743711154), ('Ross', 0.004143237644273453), ('Dish', 0.0038472920982539215), ('Ooh', 0.0032554010062148565), ('Cash', 0.0032554010062148565), ('Dash', 0.0026635099141757916), ('Lash', 0.0023675643681562593), ('Bush', 0.0023675643681562593), ('Rash', 0.0020716188221367266), ('Moss', 0.0017756732761171944), ('Hush', 0.0017756732761171944), ('Boss', 0.001479727730097662), ('Ash', 0.001479727730097662), ('Toss', 0.0011837821840781297), ('Hoch', 0.0011837821840781297), ('Sash', 0.0008878366380585972), ('Pooh', 0.0008878366380585972), ('Os', 0.0008878366380585972), ('Los', 0.0008878366380585972), ('Foch', 0.0008878366380585972), ('Ouh', 0.0005918910920390648), ('Mash', 0.0005918910920390648), ('Gash', 0.0005918910920390648), ('Vos', 0.0002959455460195324), ('Vish', 0.0002959455460195324), ('Roth', 0.0002959455460195324), ('Rosa', 0.0002959455460195324), ('Oooh', 0.0002959455460195324), ('Nos', 0.0002959455460195324), ('Noah', 0.0002959455460195324), ('Mesh', 0.0002959455460195324), ('Lush', 0.0002959455460195324), ('Jose', 0.0002959455460195324), ('Gush', 0.0002959455460195324), ('Gosp', 0.0002959455460195324), ('Foh', 0.0002959455460195324), ('Doth', 0.0002959455460195324), ('Dost', 0.0002959455460195324), ('Cosy', 0.0002959455460195324), ('Cos', 0.0002959455460195324)]\n",
      "Billings:\t[('Willings', 1.0)]\n",
      "Shaw:\t[('Thaw', 0.5), ('Shaw', 0.5)]\n",
      "Nasby:\t[('Easy', 0.6421052631578947), ('Baby', 0.23684210526315788), ('Nasty', 0.06315789473684211), ('Hasty', 0.03684210526315789), ('Ashy', 0.010526315789473684), ('Pasty', 0.005263157894736842), ('Cabby', 0.005263157894736842)]\n",
      "Ross:\t[('Loss', 0.8313953488372093), ('Ross', 0.08139534883720931), ('Moss', 0.03488372093023256), ('Boss', 0.029069767441860465), ('Toss', 0.023255813953488372)]\n",
      "Ward:\t[('Hard', 0.5960264900662252), ('Yard', 0.26158940397350994), ('Card', 0.09933774834437085), ('Ward', 0.04304635761589404)]\n",
      "Farrar:\t[('Tartar', 0.5882352941176471), ('Array', 0.4117647058823529)]\n",
      "Browne:\t[('Grown', 0.2849002849002849), ('Brown', 0.2022792022792023), ('Crown', 0.1737891737891738), ('Frowned', 0.1225071225071225), ('Frown', 0.06552706552706553), ('Drowned', 0.039886039886039885), ('Prone', 0.03418803418803419), ('Crowned', 0.02849002849002849), ('Drown', 0.02564102564102564), ('Drone', 0.008547008547008548), ('Crowns', 0.005698005698005698), ('Rowe', 0.002849002849002849), ('Frowde', 0.002849002849002849), ('Drowns', 0.002849002849002849)]\n",
      "As:\t[('Is', 0.4043019648397104), ('As', 0.33356773526370215), ('S', 0.23313340227507756), ('Us', 0.028293691830403308), ('Vs', 0.00037228541882109616), ('Os', 0.00012409513960703205), ('Es', 0.00012409513960703205), ('Ms', 8.27300930713547e-05)]\n",
      "Shaw:\t[('Thaw', 0.5), ('Shaw', 0.5)]\n",
      "Browne:\t[('Grown', 0.2849002849002849), ('Brown', 0.2022792022792023), ('Crown', 0.1737891737891738), ('Frowned', 0.1225071225071225), ('Frown', 0.06552706552706553), ('Drowned', 0.039886039886039885), ('Prone', 0.03418803418803419), ('Crowned', 0.02849002849002849), ('Drown', 0.02564102564102564), ('Drone', 0.008547008547008548), ('Crowns', 0.005698005698005698), ('Rowe', 0.002849002849002849), ('Frowde', 0.002849002849002849), ('Drowns', 0.002849002849002849)]\n",
      "humorists:\t[('humorist', 1.0)]\n",
      "mendacity:\t[('tenacity', 0.75), ('mentality', 0.25)]\n",
      "Mark:\t[('Dark', 0.7327935222672065), ('Mark', 0.15384615384615385), ('Park', 0.06072874493927125), ('Bark', 0.03643724696356275), ('Hark', 0.008097165991902834), ('Lark', 0.004048582995951417), ('Ark', 0.004048582995951417)]\n",
      "Twain:\t[('Swain', 0.6666666666666666), ('Twain', 0.3333333333333333)]\n",
      "imitator:\t[('imitators', 1.0)]\n",
      "Browne:\t[('Grown', 0.2849002849002849), ('Brown', 0.2022792022792023), ('Crown', 0.1737891737891738), ('Frowned', 0.1225071225071225), ('Frown', 0.06552706552706553), ('Drowned', 0.039886039886039885), ('Prone', 0.03418803418803419), ('Crowned', 0.02849002849002849), ('Drown', 0.02564102564102564), ('Drone', 0.008547008547008548), ('Crowns', 0.005698005698005698), ('Rowe', 0.002849002849002849), ('Frowde', 0.002849002849002849), ('Drowns', 0.002849002849002849)]\n",
      "humorists:\t[('humorist', 1.0)]\n",
      "Nor:\t[('For', 0.5520120884364562), ('Or', 0.42564020995705426), ('Nor', 0.022268172419277876), ('Cor', 7.95291872117067e-05)]\n",
      "humorists:\t[('humorist', 1.0)]\n",
      "Bill:\t[('Will', 0.6934916446789797), ('Till', 0.09498680738786279), ('Ill', 0.06068601583113457), ('Hill', 0.047053649956024624), ('Bill', 0.04353562005277045), ('Kill', 0.025065963060686015), ('Fill', 0.01802990325417766), ('Mill', 0.0079155672823219), ('Sill', 0.007475813544415127), ('Vill', 0.0013192612137203166), ('Pill', 0.00043975373790677223)]\n",
      "Nye:\t[('Eye', 0.7432432432432432), ('Rye', 0.12162162162162163), ('Ye', 0.05405405405405406), ('Bye', 0.0472972972972973), ('Aye', 0.02027027027027027), ('Dye', 0.013513513513513514)]\n",
      "Nye:\t[('Eye', 0.7432432432432432), ('Rye', 0.12162162162162163), ('Ye', 0.05405405405405406), ('Bye', 0.0472972972972973), ('Aye', 0.02027027027027027), ('Dye', 0.013513513513513514)]\n",
      "creditably:\t[('creditable', 1.0)]\n",
      "Mr:\t[('Or', 0.9176954732510288), ('Mr', 0.06172839506172839), ('R', 0.009087791495198902), ('Dr', 0.00823045267489712), ('Fr', 0.0008573388203017832), ('Er', 0.0008573388203017832), ('Gr', 0.0006858710562414266), ('Jr', 0.0003429355281207133), ('Tr', 0.00017146776406035664), ('Br', 0.00017146776406035664), ('Ar', 0.00017146776406035664)]\n",
      "Dooley:\t[('Woolen', 0.4411764705882353), ('Volley', 0.11764705882352941), ('Motley', 0.08823529411764706), ('Coolly', 0.08823529411764706), ('Cooled', 0.08823529411764706), ('Fooled', 0.058823529411764705), ('Coley', 0.058823529411764705), ('Woolly', 0.029411764705882353), ('Morley', 0.029411764705882353)]\n",
      "Finley:\t[('Finely', 0.55), ('Dingley', 0.25), ('Ripley', 0.15), ('Linsey', 0.05)]\n",
      "Dunne:\t[('June', 0.3013698630136986), ('Funny', 0.17123287671232876), ('Tune', 0.10273972602739725), ('Ounce', 0.08904109589041095), ('Anne', 0.0684931506849315), ('Une', 0.0547945205479452), ('Gunner', 0.0547945205479452), ('Sunny', 0.0410958904109589), ('Tunnel', 0.0273972602739726), ('Runner', 0.02054794520547945), ('Funke', 0.02054794520547945), ('Unna', 0.0136986301369863), ('Bonne', 0.0136986301369863), ('Tonne', 0.00684931506849315), ('Funnel', 0.00684931506849315), ('Donne', 0.00684931506849315)]\n",
      "Ade:\t[('De', 0.9957983193277311), ('Ode', 0.004201680672268907)]\n",
      "Ade:\t[('De', 0.9957983193277311), ('Ode', 0.004201680672268907)]\n",
      "Mr:\t[('Or', 0.9176954732510288), ('Mr', 0.06172839506172839), ('R', 0.009087791495198902), ('Dr', 0.00823045267489712), ('Fr', 0.0008573388203017832), ('Er', 0.0008573388203017832), ('Gr', 0.0006858710562414266), ('Jr', 0.0003429355281207133), ('Tr', 0.00017146776406035664), ('Br', 0.00017146776406035664), ('Ar', 0.00017146776406035664)]\n",
      "Dooley:\t[('Woolen', 0.4411764705882353), ('Volley', 0.11764705882352941), ('Motley', 0.08823529411764706), ('Coolly', 0.08823529411764706), ('Cooled', 0.08823529411764706), ('Fooled', 0.058823529411764705), ('Coley', 0.058823529411764705), ('Woolly', 0.029411764705882353), ('Morley', 0.029411764705882353)]\n",
      "Archey:\t[('Arched', 0.7777777777777778), ('Arches', 0.1111111111111111), ('Archery', 0.1111111111111111)]\n",
      "Fables:\t[('Tables', 0.95), ('Gables', 0.05)]\n",
      "Slang:\t[('Clang', 0.7142857142857143), ('Slang', 0.2857142857142857)]\n",
      "This:\t[('His', 0.7117826487905228), ('This', 0.28821735120947717)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#12.7\r\n",
    "\r\n",
    "#installs textatistic if not installed\r\n",
    "#!pip install textatistic\r\n",
    "\r\n",
    "from textatistic import Textatistic\r\n",
    "\r\n",
    "article1 = requests.get('https://news.mit.edu/2022/neurodegenerative-disease-can-progress-newly-identified-patterns-0927').content\r\n",
    "article2 = requests.get('https://news.mit.edu/2022/new-program-support-translational-research-ai-data-science-machine-learning-0927').content\r\n",
    "article3 = requests.get('https://news.mit.edu/2022/understanding-reality-through-algorithms-fernanda-de-la-torre-0925').content\r\n",
    "\r\n",
    "articles = [article1, article2, article3]\r\n",
    "\r\n",
    "#performs readability assessments on each article\r\n",
    "for article in articles:\r\n",
    "    soup = BeautifulSoup(article, 'html.parser')\r\n",
    "    text = soup.get_text(strip = True)\r\n",
    "    readability = Textatistic(text)\r\n",
    "    print(readability.dict())\r\n",
    "    print(f\"Average Words per sentence: {readability.word_count / readability.sent_count:.3f}\")\r\n",
    "    print(f\"Average Characters per word: {readability.char_count / readability.word_count:.3f}\")\r\n",
    "    print(f\"Average Syllables per word: {readability.sybl_count / readability.word_count:.3f}\\n\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'char_count': 12298, 'word_count': 1862, 'sent_count': 68, 'sybl_count': 3318, 'notdalechall_count': 931, 'polysyblword_count': 377, 'flesch_score': 28.288528306059305, 'fleschkincaid_score': 16.116185316231757, 'gunningfog_score': 19.051759651228913, 'smog_score': 16.58030385493651, 'dalechall_score': 12.889664705882353}\n",
      "Average Words per sentence: 27.382\n",
      "Average Characters per word: 6.605\n",
      "Average Syllables per word: 1.782\n",
      "\n",
      "{'char_count': 7341, 'word_count': 1061, 'sent_count': 32, 'sybl_count': 1949, 'notdalechall_count': 546, 'polysyblword_count': 226, 'flesch_score': 17.775751207587206, 'fleschkincaid_score': 19.016903569745523, 'gunningfog_score': 21.782763901979266, 'smog_score': 18.31093147301405, 'dalechall_score': 13.406723892554194}\n",
      "Average Words per sentence: 33.156\n",
      "Average Characters per word: 6.919\n",
      "Average Syllables per word: 1.837\n",
      "\n",
      "{'char_count': 12128, 'word_count': 2069, 'sent_count': 76, 'sybl_count': 3357, 'notdalechall_count': 824, 'polysyblword_count': 353, 'flesch_score': 41.9375182836865, 'fleschkincaid_score': 14.173007745923535, 'gunningfog_score': 17.71402660832846, 'smog_score': 15.441011706490631, 'dalechall_score': 11.275320594744473}\n",
      "Average Words per sentence: 27.224\n",
      "Average Characters per word: 5.862\n",
      "Average Syllables per word: 1.623\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#12.8\r\n",
    "\r\n",
    "#=====will install spacy if not installed=======\r\n",
    "# !pip install -U pip setuptools wheel\r\n",
    "# !pip install spacy\r\n",
    "# !python -m spacy download en_core_web_sm\r\n",
    "\r\n",
    "import spacy\r\n",
    "\r\n",
    "response = requests.get('https://www.newscientist.com/article/2327766-deepmind-ai-learns-physics-by-watching-videos-that-dont-make-sense/')\r\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\r\n",
    "text = soup.get_text(strip = True)\r\n",
    "\r\n",
    "\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "document = nlp(text)\r\n",
    "\r\n",
    "for entity in document.ents:\r\n",
    "    print(f'{entity.text}: {entity.label_}')\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AI: ORG\n",
      "DeepMind: ORG\n",
      "July 2022ByMatthew: DATE\n",
      "ShutterstockTeachingartificial: ORG\n",
      "DeepMind: ORG\n",
      "UK: GPE\n",
      "DeepMind: ORG\n",
      "Luis Pilotoat DeepMind: PERSON\n",
      "Physics Learning: LOC\n",
      "PLATO: ORG\n",
      "PLATO: ORG\n",
      "PLATO: ORG\n",
      "PLATO: ORG\n",
      "five: CARDINAL\n",
      "AI: ORG\n",
      "AI: GPE\n",
      "new contexts: GPE\n",
      "the University of Southampton: ORG\n",
      "UK: GPE\n",
      "AI: ORG\n",
      "Fengat New York University: ORG\n",
      "AI: ORG\n",
      "first: ORDINAL\n",
      "10.1038: CARDINAL\n",
      "January: DATE\n",
      "cobStealth: ORG\n",
      "DelhiComputer: PRODUCT\n",
      "their 80s: DATE\n",
      "Beirut: PERSON\n",
      "NASA: ORG\n",
      "Apollo: ORG\n",
      "NASA: ORG\n",
      "their 80s: DATE\n",
      "Beirut: PERSON\n",
      "Juno: ORG\n",
      "Europa: LOC\n",
      "20: CARDINAL\n",
      "yearsSpaceX: GPE\n",
      "the Hubble Space Telescope: FAC\n",
      "issue3395: GPE\n",
      "16 July 2022SubscribeView: QUANTITY\n",
      "Jamaica: GPE\n",
      "decadeAdvertisementMORE: GPE\n",
      "NEW SCIENTISTComputer: ORG\n",
      "Beirut: GPE\n",
      "usPrivacy policyCookie policyTerms & conditionsAdvertiseWrite: ORG\n",
      "jobsSyndicationRSS: PERSON\n",
      "SettingsGet: ORG\n",
      "USInstagram: ORG\n",
      "Copyright New Scientist Ltd.: ORG\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#12.10\r\n",
    "\r\n",
    "text1 = requests.get('https://www.gutenberg.org/cache/epub/10947/pg10947.txt').text\r\n",
    "text2 = requests.get('https://www.gutenberg.org/cache/epub/1531/pg1531.txt').text\r\n",
    "\r\n",
    "document1 = nlp(text1)\r\n",
    "document2 = nlp(text2)\r\n",
    "\r\n",
    "document1.similarity(document2)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Etern\\AppData\\Local\\Temp\\ipykernel_23324\\3513313808.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  document1.similarity(document2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8218026208310678"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}